{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fff14fb",
   "metadata": {},
   "source": [
    "# Clustering template for Seaborn example datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596183ca",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "We are going to build a k-means clustering model and evaluate its performance. For clustering we need several independent variables. These are the variables we are going to use to identify the clusters. For clustering all of these need to be numerical and at least some need to be continuous.\n",
    "\n",
    "For a clustering model to provide useful groups, we need our independent variables to:\n",
    "\n",
    "- Be independent of each other. If the independent variables are highly correlated then we only need to use one of them to get the same information.\n",
    "- Have a similar scale. If some have much smaller values or a smaller range of values than the others they will not be taken into account when building the clustering model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54a7e4",
   "metadata": {},
   "source": [
    "<a id='Contents'></a>\n",
    "## Contents\n",
    "In this notebook, we will:<b>\n",
    "\n",
    "- [Import](#import) packages and load in some data \n",
    "- [Prepare](#prepare) the data so we can explore it\n",
    "- [Explore](#explore) the data and make our testable hypothesis\n",
    "- [Build](#build) the model \n",
    "- [Interpret](#interpret) the model results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f43a5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Reminder:</b> <br>\n",
    "You don't need to understand all the code here for now, just look for:\n",
    "<ul>\n",
    "<li> What we are trying to do with each code cell. How does it fit in our objective?\n",
    "<li> What the outputs of each code cell tell us? Are we reading too much into the results of each code cell?\n",
    "<li> Some of the code cells will have parts that you will need to change to fit your data. You will be told what to change in the comment before the code cell.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d28584",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "## Import packages and read data\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Let's start by importing the Python packages we will need:\n",
    "- [**pandas**](https://pandas.pydata.org/): a tabular data manipulation package\n",
    "- [**seaborn**](https://seaborn.pydata.org/): a data visualisation package\n",
    "- [**scikit-learn**](https://scikit-learn.org/): a model building package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409f372",
   "metadata": {},
   "source": [
    "We will be using one of these Seaborn [example datasets](https://github.com/mwaskom/seaborn-data):\n",
    "\n",
    "- `taxis`: New York taxi journeys \n",
    "- `titanic`: Records of details of passengers on the Titanic\n",
    "- `penguins`: Physical details of various penguins\n",
    "- `iris`: Measurements of different Iris flowers\n",
    "- `tips`: Restaurant bills and tips\n",
    "\n",
    "These can be loaded using the Seaborn function [`load_dataset`](https://seaborn.pydata.org/generated/seaborn.load_dataset.html). We can have a look at the first few rows using the method [`head`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ee6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('iris')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf46257",
   "metadata": {},
   "source": [
    "<a id=\"prepare\"></a>\n",
    "### Prepare the data\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "For this notebook we are assuming the data has been prepared before being loaded in. However, it is always important to check that you have what you expect.\n",
    "\n",
    "We can look at what kind of data our table contains using the [`info`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) method. \n",
    "\n",
    "We should be asking ourselves:\n",
    "\n",
    "- Does the data contain the columns I expect?\n",
    "- Do the columns have the data type I would expect?\n",
    "- Do any of the columns have missing values? Are these in any columns we intend to use? There cannot be any null values in the rows we plan to use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d124d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7df1c",
   "metadata": {},
   "source": [
    "If we have any columns with null values that we want to use in our model, we will need to drop those pieces of data. Pandas gives us the [`dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) method to do this. If you only need to drop null values from one column you can use the `subset` parameter to pass a list of the columns you wish to be checked for null values.\n",
    "\n",
    "This piece of code has been commented out, so it will not run. To use this, remove the `#` from the start of the line and replace `'column name'` with the column that you wish to be checked for null values. You can examine more than one column at a time by listing all the columns in the square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f74e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.dropna(subset=['column name', ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db85baf",
   "metadata": {},
   "source": [
    "If one of the columns is only useful as a unique row identifier, we can use it as an index. We can set it using the [`set_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html) method. Replace `'index column name'` with the column that you wish to set as the index. If you need this, remove the `#` to uncomment the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e66f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.set_index('ID column name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea8f8a",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"explore\"></a>\n",
    "### Exploring the data\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Now that we have checked our data is clean, we can explore it. A good starting point is to look at the descriptive statistics and check that they seem reasonable. We can do so using the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) method.\n",
    "\n",
    "We should be asking ourselves:\n",
    "\n",
    "- Does the data have unexpected outliers (looking at the max and min values) that might suggest a data quality issue?\n",
    "- Is the spread of the data what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20408ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef70c6",
   "metadata": {},
   "source": [
    "It is helpful to do these checks visually. Histograms will help us see the distribution of a variable and scatter plots will show us the relationship between variables. Seaborn allows us to plot these using the functions [`histplot`](https://seaborn.pydata.org/generated/seaborn.histplot.html) for histograms and [`scatterplot`](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) for scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f447e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data, x='variable name 1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1192b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x='variable name 1', y='variable name 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f95d9",
   "metadata": {},
   "source": [
    "It can be helpful to plot all the histograms and scatter plots in one go. Seaborn allows us to do this using a [`pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5b60d",
   "metadata": {},
   "source": [
    "The scatter plots help us to understand what kind of relationships there are between our variables and how strong they are. We can also quantify this using the Pandas method [`corr`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d86d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr(numeric_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adb3cc",
   "metadata": {},
   "source": [
    "### Confirm your hypothesis\n",
    "\n",
    "Now that you have explored your data and seen the relationships in it, you can pick the independent variables you want to use build your clusters. Remember, for your independent variables to be useful in predicting the clusters, they need to:\n",
    "\n",
    "- Be independent of each other. \n",
    "- Have a similar scale. As this is quite rare, we will need to normalise the data before building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd6607",
   "metadata": {},
   "source": [
    "#### Normalising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa31207b",
   "metadata": {},
   "source": [
    "We will need to keep only the columns we want to use for clustering. We can start by listing all the columns available using the [`columns`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b465f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec31134",
   "metadata": {},
   "source": [
    "Our predictor variables, the independent variables, are likely to be a subset of our data, so we will make a list of the ones we wish to use. Replace and remove the variable names as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0628d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variable_names = ['variable name 1', \n",
    "                              'variable name 2', \n",
    "                              'variable name 3', \n",
    "                              'variable name 4', \n",
    "                              'variable name 5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ef8bd",
   "metadata": {},
   "source": [
    "With this list we can make a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2468499",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data.loc[:, independent_variable_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e01976",
   "metadata": {},
   "source": [
    "Now we are ready to normalise the data. We will use the scikit-learn [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) function. This will transform the minimum of all the variables to 0 and the maximum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_data_table = MinMaxScaler().fit_transform(data_subset[independent_variable_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7048f4d",
   "metadata": {},
   "source": [
    "To turn this array into a table we will transform it into a Pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and pass the information from our unnormalised DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43970a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalised = pd.DataFrame(normalised_data_table)\n",
    "data_normalised.columns = independent_variable_names\n",
    "data_normalised.index = data_subset.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036ed17",
   "metadata": {},
   "source": [
    "We can check what this had done to our data using a pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29648f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_normalised);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1169f7a",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"build\"></a>\n",
    "## Build the model\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Now we have our data in a form we can build our k-means clustering model. To build it, we will use the scikit-learn [`KMeans`](https://scikit-learn.org/dev/modules/clustering.html#k-means) function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883c1fd",
   "metadata": {},
   "source": [
    "We will try some possible numbers of clusters and see how the Sum Squared Errors (SSE) changes. We know that as we increase the number of clusters, and if the center's of the clusters are evenly spread throughout the data, then the\n",
    "To do so, we will generate a list of possible numbers of clusters from `2` to `15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_cluster_number = range(2, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33077734",
   "metadata": {},
   "source": [
    "Now we can loop through this list, fitting the model and recording the SSE for each, which is called `inertia_` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefbfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "\n",
    "for k in potential_cluster_number:\n",
    "    km = KMeans(n_clusters=k, n_init=10).fit(data_normalised)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f78332",
   "metadata": {},
   "source": [
    "We can plot this to see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33125f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(potential_cluster_number, Sum_of_squared_distances, 'x-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of Squared Errors');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e471e",
   "metadata": {},
   "source": [
    "From this, we are looking for one or more 'elbow' in the change of SSE. Where does it begin to change more slowly? Once you have decided a number of clusters to use, enter it as the `number_of_clusters` below. The default has been set to `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 4\n",
    "cluster_model = KMeans(n_clusters=number_of_clusters, random_state=13, n_init='auto').fit(data_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae30e9",
   "metadata": {},
   "source": [
    "We can store the cluster identified for each datapoint alongside the unnormalised data. The model parameter `labels_` will provide them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd036a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset['clusters'] = cluster_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e670e16",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"interpret\"></a>\n",
    "## Interpret the model results\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "Now we have built the model we can look at the model quality and what it tells us. We need to examine:\n",
    "\n",
    "- Do any of the clusters represent a meaningful group? It may only be one or two that do.\n",
    "- Are any of the clusters too small to be useful? This might help us identify where more data is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7163e0",
   "metadata": {},
   "source": [
    "The first thing to check is how large the clusters are. \n",
    "\n",
    "- Are there some clusters that only contain a small number of outliers? \n",
    "- Are most of the data points in one cluster?\n",
    "\n",
    "We can count the number of data points in each cluster by grouping the data in our DataFrame using the [`groupby`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) and [`count`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.count.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.groupby(['clusters']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c89179",
   "metadata": {},
   "source": [
    "We can colour a pairplot with these cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77692b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_subset, hue='clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284c648",
   "metadata": {},
   "source": [
    "We can also examine the location of the centres of all the clusters by reading them from the model using `cluster_centers_` and putting them into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4255ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cluster_model.cluster_centers_, columns=independent_variable_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4589aa",
   "metadata": {},
   "source": [
    "Lastly, we can use Seaborn to plot [`violinplot`](https://seaborn.pydata.org/generated/seaborn.violinplot.html) to compare the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in independent_variable_names:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.violinplot(data=data_subset, \n",
    "                   y=col, \n",
    "                   x='clusters',\n",
    "                   inner='quartile',\n",
    "                   ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6069b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
